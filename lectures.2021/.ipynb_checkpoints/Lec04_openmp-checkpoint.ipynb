{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial to Parallel: OpenMP\n",
    "\n",
    "The simplest and most common parallel pattern is to take a serial program and convert it into a parallel program.\n",
    "\n",
    "* My code’s not running fast enough:\n",
    "  * Video: data delays produce jitter, stalls\n",
    "  * Web: page render time causes user loss, discomfort\n",
    "  * Batch processing, indexing, analysis not completing in time \n",
    "  * High-throughput finance: other models running faster and beating mine to a decision -> lose arbitrage opportunity\n",
    "* This leads to a natural software engineering process\n",
    "  * Profile code: find out what’s slow\n",
    "  * Parallelize slow part(s) only\n",
    "  * Migrate from serial implementation to parallel implementation\n",
    "* It's  not the best process\n",
    "  * Serial to parallel doesn’t produce the best designs\n",
    "  * Best parallel implementation may require a totally different design with no incremental refactoring from serial implementation\n",
    "* Just the easiest\n",
    "  * Compared to a clean-slate redesign\n",
    "\n",
    "### What is OpenMP\n",
    "\n",
    "Parallel programming environment (not language) for:\n",
    "* Master/slave and/or fork/join execution model\n",
    "* Loop parallelism patterns\n",
    "* Thread parallelism in shared-memory architectures\n",
    "But this doesn’t mean anything yet.\n",
    "\n",
    "It’s the simplest approach to parallelism\n",
    "* Write a serial program in a language that you know (C/C++ or Fortran)\n",
    "* Add directives to parallelize portions of the code\n",
    "* Get a parallel program that computes that exact same result (_serial to parallel equivalence_)\n",
    "\n",
    "What's the merit:\n",
    "* Incremental parallelism\n",
    "* Simple to Use\n",
    "* Portable (for the most part)\n",
    "\n",
    "Limitations:\n",
    "* Difficult to manage memory usage\n",
    "* No distributed capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Parallelism\n",
    "\n",
    "The fundamental Principle in OpenMP is to parallelize a _block_ and run multiple instances of the block with parallel threads.\n",
    "\n",
    "Refer to file `openmp/block.c`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The OpenMP Toolchain\n",
    "\n",
    "No toolchain\n",
    "\n",
    "* Add -fopenmp to compiler command line\n",
    "  * generate code from pragmas directives\n",
    "  * links to libopenmp\n",
    "* `#include “omp.h”` to import symbols into your source code\n",
    "\n",
    "Example compile command lines:\n",
    "  * gcc -fopenmp -O3 program.c (gcc)\n",
    "  * gcc -Xpreprocessor -fopenmp -O3 -lomp program.c (clang MacOSX)\n",
    "\n",
    "#### (Aside) Compiler Optimization\n",
    "\n",
    "All compilers, including gcc/g++, have optimization flags that must be set to get good performance.\n",
    "\n",
    "Optimization level –O*:\n",
    "  * -O0 (default) = Reduce compilation time and make debugging produce the expected results.\n",
    "  * -O1 = simple optimizations that don’t take a lot of compile time\n",
    "  * -O2 = rewrite loops, follow jump pointers, inline small functions, no time/space tradeoffs\n",
    "  * -O3 = vectorize, inline functions, branch prediction\n",
    "\n",
    "When debugging, you want to use -O0 so the code makes sense.\n",
    "\n",
    "For performance, -O3 to vectorize code to processors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Model and Hardware\n",
    "\n",
    "OpenMP is a parallel programming environment that:\n",
    "  * creates _threads_ that run on mutliple processor cores\n",
    "\n",
    "#### Shared Memory\n",
    "\n",
    "* Coherent read/write to common memory from multiple cores/processors\n",
    "  * Coherent = repeatable read, read last write, ….\n",
    "  * Abstraction that there is a single memory for all processors\n",
    "  * Data sharing by reading/writing to memory\n",
    "* Hardware that provides this abstraction are called shared memory architectures (typically in a “single machine”)\n",
    "  * Even if there are different physical memories\n",
    "  * Non-Uniform memory architectures (NUMA) are typical today\n",
    "    * with different latency and throughput from cores to memory locations\n",
    "    * but the appearance (semantics) of a single, unified memory\n",
    "    \n",
    "<img src=\"https://hpc.llnl.gov/sites/default/files/numa.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenMP onto Accelerators (no more)\n",
    "\n",
    "Accelerators are co-processors (not the main CPU)\n",
    "  * Offload compute-intensive tasks onto specialized hardware\n",
    "  * Power dense and cheaper when compared with main CPU\n",
    "  * Often limited programming models or compute capabilities\n",
    "\n",
    "Xeon-Phi **was** the Intel accelerator architecture. Cancelled in 2020.\n",
    "  * supposed to be easier to program than GPUs because it runs lightly modified x86 code\n",
    "  * programmable via OpenMP with \"offload\" compiler directives\n",
    "  * GPUs won. They became easier to program due to frameworks and dominate the ML market.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Parallelism\n",
    "\n",
    "Loop parallelism is a form of parallelism and _programming pattern_ that derives parallel tasks from the iterations of loops.\n",
    "\n",
    "* Most common use and programming pattern for OpenMP\n",
    "  * add parallel directives to a for loop\n",
    "  * OpenMP divides the loops iterations into _chunks_ assigned to threads\n",
    "* Merits of loop parallelism\n",
    "  * __Sequential equivalence__: parallel program is equivalent to a serial program (easy to write and maintain, good tools)\n",
    "  * __Refactoring__: Incremental conversion of a serial program to a parallel program (easy to test and debug)\n",
    "* Drawbacks of loop parallelism\n",
    "  * __Memory utilization__: if loop access patterns don’t match cache hierarchy, programs often require massive restructuring\n",
    "  \n",
    "### #pragma parallel for\n",
    "\n",
    "Refer to openmp/loop.c. \n",
    "\n",
    "xeus-cling (the C/C++ notebook environment) is not working with OpenMP. The following cell will not run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "{\n",
    "  #pragma omp parallel for \n",
    "  for ( int i=0; i<100; i++ )\n",
    "  {\n",
    "    std::cout << \"OMP Thread# \" << omp_get_thread_num() << \" loop variable \" << i << \"\\n\";\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenMP divided the iterations of the loops into contiguous _chunks_ assigned to threads\n",
    "  * number of threads derived from environment\n",
    "  * chunks are (by default) sequential: leads to _coalesced_ and _sequential_ memory utilization\n",
    "  \n",
    "__STOP__ And learn about the <a href=\"./Lec05_cache_hierarchy.ipynb\">Cache Hierarchy</a>.  Then start again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
