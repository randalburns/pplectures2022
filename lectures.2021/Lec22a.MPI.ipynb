{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI: Message Passing Interface\n",
    "\n",
    "MPI is the programming interface to high-performance computing (HPC), i.e. supercomputers.\n",
    "\n",
    "* Message passing parallelism\n",
    "* Cluster computing (no shared memory)\n",
    "* Process (not thread oriented)\n",
    "* Parallelism model\n",
    "  * SPMD: by definition\n",
    "* MPI environment\n",
    "  * Application programming interface\n",
    "  * Implemented in libraries\n",
    "  * Support for C/C++ and Fortran\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPMD: Single program multiple data\n",
    "\n",
    "From wikipedia “Tasks are split up and run simultaneously on multiple processors with different input in order to obtain results faster. SPMD is the most common style of parallel programming.”\n",
    "  * Asynchronous execution of the same program\n",
    "  \n",
    "<img src=\"https://www.sharcnet.ca/help/images/8/8a/SPMD_model.png\" width=512 title=\"SPMD\" />\n",
    "\n",
    "_SPMD_ is confusing, because it seems like it should be part of Flynn's taxonomy. It is not. _SPMD_ is a software programming model. SIMD is an architectural classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first MPI program\n",
    "\n",
    "* Configure the MPI environment\n",
    "* Discover yourself\n",
    "* Take some differentiated activity\n",
    "\n",
    "all demos in `./mpi` Start with `mpimsg.c`\n",
    "\n",
    "* Idioms\n",
    "  * SPMD: all processes run the same program\n",
    "    * MPI_Rank: tell yourself apart from other and customize the local processes behaviours\n",
    "    * Find neighbors, select data region, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Vision circa 1996 (Poster at Supercomputing)\n",
    "\n",
    "<img src=\"https://www.netlib.org/mpi/mpi.gif\" width=512 />\n",
    "\n",
    "The goals of the MPI process was to normalize message passing, which was previously spread across many different incompatible libraries that were often machine dependent:\n",
    "  * portable (code reuse across different hardware, software)\n",
    "  * multiple vendors\n",
    "  * extensible (value added libraries/tools/applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MPI Toolchain\n",
    "\n",
    "Build and launch scripts that wrap a compiler.\n",
    "    \n",
    "<img src=\"./images/mpiscip.png\" width=400 />\n",
    "\n",
    "* To compile an MPI program, you call the associated wrapper.\n",
    "* To run an MPI program:\n",
    "  * **debug** `mpirun` to launch MPI job on the local machine/cluster\n",
    "  * **deploy** launch through scheduler on HPC clusters (do not run on the login node)\n",
    "\n",
    "    \n",
    "```\n",
    "mpicc mpimsg.c -o mpimsg\n",
    "mpirun mpimsg\n",
    "mpirun -np 16 --oversubscribe mpimsg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HPC Scheduler\n",
    "\n",
    "Schedule many parallel jobs onto a supercomputer based on size, resources needed, priority.\n",
    "* Maui/Torque\n",
    "* SLURM\n",
    "* OGE\n",
    "\n",
    "Each with their own submission scripts. Not mpirun.\n",
    "    \n",
    "HPC systems have login nodes that you `ssh` into.  **Do not call `mpirun` on login nodes**\n",
    "  * this tries to run a parallel job on the login node.\n",
    " \n",
    "<img src=\"https://portal.tacc.utexas.edu/documents/10157/1181317/Login+and+compute+nodes/dd6fa98c-1695-4e62-8b7b-66f0c83ceba3?t=1436213020000\" width=512 />  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI Runtime\n",
    "\n",
    "MPI programs are just C/Fortran that include message passing directives.\n",
    "One designs an SPMD program that will collaborate to solve a problem that includes:\n",
    "  * Calls to the MPI library\n",
    "  * Interactions with the MPI runtime\n",
    "  \n",
    "Some calls query or manipulate the runtime:  \n",
    "* Initialize the environment\n",
    "  * `MPI_Init ( &argc, &argv )`\n",
    "* Acquire information for process to differentiate process behavior in SMPD\n",
    "  * `MPI_Comm_size ( MPI_COMM_WORLD, &num_procs )`\n",
    "  * `MPI_Comm_rank ( MPI_COMM_WORLD, &ID )`\n",
    "* And cleanup\n",
    "  * `MPI_Finalize()`\n",
    "  \n",
    "## MPI Design Ethos\n",
    "* MPI is just messaging.\n",
    "    * And synchronization constructs, which are built on messaging\n",
    "    * And library calls for discovery and configuration\n",
    "* Computation is done in C/C++/Fortran SPMD program\n",
    "* MPI is sometimes called the “assembly language” of supercomputing\n",
    "    * Simple primitives\n",
    "    * Build your own communication protocols, application topologies, parallel execution\n",
    "    * The opposite end of the design space from Dask, Spark in which you write simple declarative programs that are automatically parallelized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
