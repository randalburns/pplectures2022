{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factors against Parallelism\n",
    "\n",
    "* Startup costs associated with initiating (or tearing down) processes\n",
    "  * May often overwhelm actual processing time (rendering ||ism useless)\n",
    "  * Involve thread/process creation, data movement\n",
    "* Interference: slowdown resulting from multiple processors accessing shared resources\n",
    "  * Resources: memory, I/O, system bus, sub-processors\n",
    "  * Software synchronization: locks, latches, mutexes\n",
    "  * Hardware synchronization: cache faults, interrupts\n",
    "* Skew: when breaking a single task into many smaller tasks, not all tasks may be the same size\n",
    "  * Not all tasks finish at the same time\n",
    "  \n",
    "The course will cover all of these in specific details. At this time, you should understand these three factors by visual example. I used to teach this unit by analogy with the real world examples. This was cute, but confusing. I've left this material at the end. The visual examples are boring, but concise and accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Startup Costs\n",
    "\n",
    "The NERSC tutorial on OpenMP (our next unit) draws a schematic of a parallel program as consisting of parallel execution parts and serial parts. \n",
    "\n",
    "<img src=\"https://docs.nersc.gov/development/programming-models/openmp/OpenMPforkjoin.png\" width=\"500\" title=\"https://docs.nersc.gov/development/programming-models/openmp/OpenMPforkjoin.png\" />\n",
    "\n",
    "The portions of the progam in the parallel region during which the threads are not running fully in parallel are the startup costs (or teardown costs). The dashed lines indicate periods of time in which the program is not running in parallel. The solid lines indicate times when the program is running in parallel.\n",
    "\n",
    "During the sequential parts, one would not expect parallelism. This diagram shows that even during the parallel parts, one might not realize perfect parallelism.\n",
    "\n",
    "Examples of startup costs include:\n",
    "  * operating system overheads: creation of threads of processes\n",
    "  * data access: loading or copying data to the parallel workers\n",
    "  \n",
    "Similarly we may experience teardown costs in closing parallel contexts:\n",
    "  * operating system cleaning up thread/process state\n",
    "  * copying partial results from parallel computation to memory or storage\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interference\n",
    "\n",
    "The paper [Thiffault et al. Dynamic instrumentation of large-scale MPI and OpenMP applications](http://cs.umanitoba.ca/pub/IPDPS03/DATA/16_04_089.PDF) shows a timeline of parallel computation of a neutron-transfer physics code. The image shows multiple processes awaiting on data to be transferred from other nodes and threads within each process executing intermittently.   \n",
    "\n",
    "<img src=\"./images/sweep3d.png\" width=\"500\" /> \n",
    "\n",
    "Interference arises when parallel execution contexts have to wait on each other. This manifests in multiple ways:\n",
    "  * communicating processes waiting on messages\n",
    "  * processes waiting on shared resources\n",
    "      * these can be sofware constructs, such as locks\n",
    "      * or they can be hardware constructs, such as cache lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew\n",
    "\n",
    "Skew arises when execution cannot advance until all jobs complete. This problem is most acute when one decomposes a problem into independent parts that are of different sizes. The following chart from [Uselton et al. Parallel I/O Performance: From Events to Ensembles](https://crd.lbl.gov/assets/pubs_presos/CDS/FTG/Papers/2010/ipdps10ipm.pdf) shows a parallel code that is running particularly poorly owing to long-running processes.\n",
    "\n",
    "<img src=\"./images/ioensembles.png\" width=\"500\" />\n",
    "\n",
    "This is an I/O trace. The blue and red lines indicate periods when I/O is being conducted and the white space indicates all else. In this code, all I/O must complete prior to a barrier prior to the computation continuing. The chart shows processes completing waiting for the long running processes. \n",
    "\n",
    "Long running processes are called. \"stragglers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Factors\n",
    "\n",
    "Many computational frameworks operate in a manner in which they:\n",
    "  * launch a bunch of parallel jobs (startup)\n",
    "  * wait for all jobs to complete (skew)\n",
    "  * integrate/coordinate the parts (interference)\n",
    "  * restart a bunch of new parallel jobs -- REPEAT!\n",
    "  \n",
    "  \n",
    "The following diagram of the bulk synchronous computing model shows skew and synchronization. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/Bsp.wiki.fig1.svg/540px-Bsp.wiki.fig1.svg.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Computers: Real things that Degrade Parllelism\n",
    "\n",
    "* I/O (memory and storage)\n",
    "  * may be startup (load data before computation)\n",
    "  * may be interference (awaiting data transfer between parallel tasks)\n",
    "  * may be skew (await I/O completion of one task)\n",
    "* Network communication\n",
    "    * similar to I/O but always involves communication\n",
    "* Failures—particularly slow/failed processes (often skew)\n",
    "\n",
    "The HPC community focuses on communication (among processes) as the major source of slowdown.  This is a traditional (I/O and networking) view.\n",
    "\n",
    "### Communication\n",
    "\n",
    "* Parallel computation proceeds in phases\n",
    "  * Compute (evaluate data that you have locally)\n",
    "* Communicate (exchange data among compute tasks).  Performance is governed by:\n",
    "  * Latency: fixed cost to send a message\n",
    "  * Round trip time (speed of light and switching costs)\n",
    "* Bandwidth: marginal cost to send a message\n",
    "  * Link capacity\n",
    "* Latency dominates small messages and bandwidth dominates large.\n",
    "  * It is lmost always better to increase message size for performance, but difficult to achieve in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigation: Overlapped I/O and Computation\n",
    "\n",
    "(I/O or messaging) and computation that occur in parallel are overlapped\n",
    "\n",
    "<img src=\"./images/overlap.png\" width=\"512\" title=\"Unknown source\" />\n",
    "\n",
    "* _Concept_: When performing a slow operation\n",
    "  * do the slow operation asynchronously\n",
    "  * do useful work with processor while waiting\n",
    "* Overlap is one of the simplest and most important forms of asynchronous execution\n",
    "  * identify independent tasks and do in parallel\n",
    "  * reorder I/O to initiate as early as possible and wait as late as possible\n",
    "  * while computing at the same time\n",
    "  \n",
    "I've built a toy example to demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute \n",
    "def factorial(number):  \n",
    "    f = 1\n",
    "    for i in range(2, number+1):\n",
    "        f *= i\n",
    "    return f\n",
    " \n",
    "# synchronous I/O\n",
    "def io_from_devnull(number):\n",
    "    with open(\"/dev/null\", \"rb\") as fh:\n",
    "        for i in range(number):\n",
    "            fh.read(1)\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4 ms ± 109 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "22 ms ± 29.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit factorial(10000)\n",
    "%timeit io_from_devnull(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.4 ms ± 107 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 20 \n",
    "factorial(10000)\n",
    "io_from_devnull(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.2 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 20 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 20\n",
    "\n",
    "import paralleldefs\n",
    "\n",
    "from multiprocessing import Process\n",
    "p1 = Process(target=paralleldefs.factorial, args=(10000,))\n",
    "p2 = Process(target=paralleldefs.io_from_devnull, args=(30000,))\n",
    "p1.start()\n",
    "p2.start() \n",
    "p1.join()\n",
    "p2.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factors Conclusions\n",
    "\n",
    "* Factors against parallelism are the most important design consideration.\n",
    "* Considered in light of Amdahl's law\n",
    "    * when we estimate an Amdahl number from empirical results all factors contribute to the unoptimized portion\n",
    "    * when we implement a parallel portion from an instrumented serial version, the factors are why the realized speedup is less than the ideal speedup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
