{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map/Reduce Semantics and Systems\n",
    "\n",
    "### Types and transformations\n",
    "* Map is a transformation\n",
    "  * Input domain to output domain\n",
    "* Reduce is a collection\n",
    "  * No domain change\n",
    "  \n",
    "$$\n",
    "map (k1, v1) \\rightarrow list(k2,v2) \\\\\n",
    "reduce (k2, list(v2)) \\rightarrow list(v2)\n",
    "$$\n",
    "\n",
    "* Google C++ implementation is based all on strings\n",
    "  * User code must convert to structured types\n",
    "* Hadoop! has type wrappers\n",
    "\n",
    "### Parallelism in Map/Reduce\n",
    "\n",
    "* How much potential paralleism in mappers? in reducers?\n",
    "\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "......spoiler alert.......\n",
    "  \n",
    ".  \n",
    "\n",
    ".\n",
    "* Mappers: up to a parallel process for each input (typically a file)\n",
    "* Reducers: up to a parallel process for each key\n",
    "* So for the WordCount example\n",
    "  * two files = two mappers\n",
    "  * 5 different words = five reducers\n",
    "  * but this is scalable with input\n",
    "\n",
    "\n",
    "Differentiating betweens mapper/reducer and map/reduce processes\n",
    "* A cluster will typically configure number of available phyical processes\n",
    "  * this numer is typically much smaller than potential parallelism\n",
    "  * we refer to `mappers` as potential parallelism and `map processes` as the number of phyical processes running map functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map/Reduce Runtimes\n",
    "\n",
    "From the Google paper https://www.usenix.org/legacy/events/osdi04/tech/dean.html\n",
    "\n",
    "<img src=\"images/mr.png\" width=512 />\n",
    "\n",
    "* Automatically partition input data\n",
    "  * 16-64 MB chunks for Google\n",
    "* Create M map tasks: one for each chunk\n",
    "  * Assign available workers (up to M) to tasks\n",
    "* Write intermediate pairs to local (to worker) disk\n",
    "* R reduce tasks (defined by user) read and process intermediate results\n",
    "* Output is up to R files available on shared file system\n",
    "* Master tracks state\n",
    "  * Asssignment of M map tasks and R reduce tasks to workers\n",
    "  * State and liveliness of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systems Issues\n",
    "\n",
    "The map/reduce runtime must deal with:\n",
    "* Master failure\n",
    "  * Checkpoint/restart, classic distributed systems/replication problem\n",
    "* Failed worker\n",
    "  * Heartbeat liveness detection, restart\n",
    "* Slow worker\n",
    "  * Backup tasks\n",
    "* Locality of processing to data\n",
    "  * Big deal, they don’t really solve\n",
    "  * But, much subsequent research does\n",
    "* Task granularity\n",
    "  * Metadata size and protocol scaling (not inherent parallelism) limit the size of M and R\n",
    "  \n",
    "### Google File System: The Data Service\n",
    "\n",
    "* Goals\n",
    "  * Wide-distribution\n",
    "  * Commodity hardware\n",
    "  * High (aggregate) performance\n",
    "* Different assumptions than traditional file systems\n",
    "  * Component failures are normal behavior\n",
    "  * Files are huge (new to Google environment ca. 2004)\n",
    "* Most files have append-only writing, \n",
    "  * Mandate append-only writing to realize good I/O properties\n",
    "  * Why append only?\n",
    "      * reduce contention -- logical locking of tail rather than physcial locking of offset\n",
    "      * no data reorganization for writes\n",
    "\n",
    "GFS architecture (from https://www.cs.rutgers.edu/~pxk/417/notes/16-dfs.html)\n",
    "    \n",
    "<img src=\"images/gfs.png\" width=512 />\n",
    "\n",
    "<img src=\"images/gfs2.png\" width=512 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Reliable services\n",
    "  * Master, scheduler, lock services fault tolerant\n",
    "* Data are triple replicated\n",
    "  * On nodes that have independent failure properties (different racks, power supplies, networks)\n",
    "  * This became standard practice in cloud key/value stores (for a decade, now supersedes by distributed error coding)\n",
    "  * Tolerates two failures\n",
    "  * Rereplicated on failure detection\n",
    "  \n",
    "Here is the originial diagram from https://www.usenix.org/legacy/events/osdi04/tech/dean.html\n",
    "\n",
    "<img src=\"images/gfs.orig.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GFS is an **out of band** file system.\n",
    "  * metadata and data are seperate coordinated services\n",
    "\n",
    "### How GFS changed the world\n",
    "\n",
    "* Atomic checkpoint and append\n",
    "  * Major mode for writing\n",
    "  * Great semantics for limited usage\n",
    "* Abandon POSIX file system semantics\n",
    "* In-memory metadata at Master\n",
    "  * Gotta keep it small\n",
    "  * Even for big data (scale metadata memory in proportion to aggregate storage)\n",
    "* Re-replication\n",
    "  * Keep three, detect missing on read/write\n",
    "  * Forget reliable storage, forget RAID\n",
    "* Design for failures (not recovery)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantics\n",
    "\n",
    "Let's start with some definitions:\n",
    "\n",
    "#### Shuffle\n",
    "\n",
    "This is the routing process of mapper outputs to reducer inputs.\n",
    "\n",
    "<img src=\"images/hadoopsem.png\" width=512 />\n",
    "\n",
    "#### Partition\n",
    "\n",
    "* Partition is the output file of a reducer _process_ (not a single reducer).\n",
    "  * Contains many reducer keys\n",
    " \n",
    "#### Combiner\n",
    "\n",
    "This image is linked from https://data-flair.training/blogs/hadoop-combiner-tutorial/.  Please refer to their page.\n",
    "\n",
    "<img src=\"https://d2h0cx97tjks2p.cloudfront.net/blogs/wp-content/uploads/sites/2/2017/09/mapreduce-program-with-combiner.jpg\" width=512 />\n",
    "\n",
    "\n",
    "\n",
    "Combiner is a function that runs on the outputs of the mapper before the shuffle.\n",
    "\n",
    "* Combiner executes on the mappers $ \\langle key,value \\rangle$ output while in memory at the mapper\n",
    "  * It is possible to write unique combiner and reduce classes\n",
    "  * It is common to use the reducer as a combiner\n",
    "  * Combiner must have algebraic propertics, i.e. `reduce(combine(A),combine(B)) == reduce(A,B)` \n",
    "  * Traditional reduce operators (aggregates and extrema) work in combiners\n",
    "* Combiner in WordCount:\n",
    "  * compute sum output by mapper for each key and send a single aggregated value to reducers\n",
    "* Combiner for Maximum: \n",
    "  * compute maximum value for each key.  \n",
    "  * Reducer computes a maximum of maxima.\n",
    "* __Caution!!!__ Your homework assignment cannot use a combiner.  I will ask you why?\n",
    "\n",
    "\n",
    "### The Map/Reduce Sorting Guarantee\n",
    "\n",
    "* Map: extracts a sorting key from the value\n",
    "$$\n",
    " \\langle key, value \\rangle -> \\langle sort\\_key, output\\_value \\rangle\n",
    "$$\n",
    "* Shuffle does not sort strictly:\n",
    "  * it route's to reducer based on sort key.\n",
    "  * typically hashing maps key to reducer\n",
    "  * Keys are sorted as they are presented to the reducer\n",
    "\n",
    "Here is the guarantee:\n",
    "\n",
    "_We guarantee that within a given partition, the intermediate key/value pairs are processed in increasing key order. This ordering guarantee makes it easy to generate\n",
    "a sorted output file per partition, which is useful when\n",
    "the output file format needs to support efficient random\n",
    "access lookups by key, or users of the output find it convenient to have the data sorted._\n",
    "\n",
    "The ordering guarantees sort within partitions\n",
    "* To sort completely:\n",
    "  * All output to a single partition (use one reducer)\n",
    "  * Customize the shuffle function (quite complex and can introduce skew)\n",
    "  * The default shuffle uses hashing (for load balance)\n",
    "\n",
    "* The Google paper optimizes sort by customizing shuffle so that partitions are ordered, not randomized\n",
    "  * Run a M/R job to learn the key distribution\n",
    "  * Specify a shuffle based on evenly paritioning the key distribution\n",
    "  * This is also how Hadoop!’s sort record worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
