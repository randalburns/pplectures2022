{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI Messaging and Deadlock\n",
    "\n",
    "### Point-to-Point Messaging\n",
    "\n",
    "* This is the fundamental operation in MPI\n",
    "* Send a message from one process to another\n",
    "  * Blocking I/O\n",
    "  * Blocking provides built in synchronization\n",
    "  * Blocking can lead to deadlock\n",
    "* Send and receive, let's do an example in `nodeadlock.c`\n",
    "  * this program passes a \"token\" around a ring of processes\n",
    "  * ring defined by the program\n",
    "  * **NOTE** this is a flawed program.  The correct implementation is `passitforward.c`\n",
    "  \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/36/MPI_Ring_topology.png\" width=312 />\n",
    "\n",
    "  \n",
    "* What's in a message?\n",
    "  * first three arguments specify content\n",
    "  * then location (receive from or send to)\n",
    "  * message metadata\n",
    "  * and a \"communicator\" which is a virtual network\n",
    "  \n",
    "```c\n",
    "int MPI_Send ( \n",
    "  \tvoid* sendbuf, \n",
    "\tint count, \n",
    "\tMPI_Datatype datatype,\n",
    "\tint dest,\n",
    "\tint tag,\n",
    "\tMPI_Comm comm )\n",
    "    \n",
    "int MPI_Recv ( \n",
    "  \tvoid* recvbuf, \n",
    "\tint count, \n",
    "\tMPI_Datatype datatype,\n",
    "\tint source,\n",
    "\t. . . )\n",
    "```\n",
    "\n",
    "* All MPI data are arrays\n",
    "  * Where is it? `void *`\n",
    "  * How many? `count`\n",
    "  * What type? `MPI_Datatype`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadlock\n",
    "\n",
    "_Wikipedia_ \"In concurrent computing, a deadlock is a state in which each member of a group is waiting for another member, including itself, to take action, such as sending a message or more commonly releasing a lock.\"\n",
    "\n",
    "* Conditions for deadlock\n",
    "  * Mutual exclusion\n",
    "  * Hold and wait\n",
    "  * No preemption\n",
    "  * Circular wait\n",
    "  \n",
    "The simplest deadlock occurs with two processes and 2 resources.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Process_deadlock.svg/440px-Process_deadlock.svg.png\" width=256 />\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/df/Two_processes%2C_two_resources.gif\" width=256 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the Giphy, the deadlock is resolved by restarting the top process.  This violates the \"hold and wait\" principle.\n",
    "\n",
    "In MPI, programmers design their own messaging protocols.  We note that:\n",
    "  * MPI messaging is synchronous and blocking\n",
    "  * i.e. the caller waits on the message to be delivered prior to returning\n",
    "So why didn't nodeadlock.c produce a deadlock?\n",
    "\n",
    "Examples:\n",
    "* `deadlock.c` shows the flaw in `nodeadlock.c`\n",
    "* `passitforward.c` correct implementation with _paired send and receive_\n",
    "\n",
    "### Standard Mode MPI\n",
    "\n",
    "* A blocking standard send may be implemented by the MPI runtime in a variety of ways\n",
    "  * ` MPI_Send( …, MPI_COMM_WORLD )`\n",
    "* Can be buffered at sender or receiver\n",
    "  * depending upon message size, number of processes\n",
    "* Converting to a mandatory synchronous send reveals the deadlock\n",
    "  * `MPI_Ssend( …, MPI_COMM_WORLD )`\n",
    "  * But so could increasing the # of processors\n",
    "* MPI runtime chooses best behavior for messaging based on system/message parameters:\n",
    "  * Amount of buffer space\n",
    "  * Message size\n",
    "  * Number of nodes\n",
    "* Preferred way to program??\n",
    "  * Commonly used and realizes good performance\n",
    "  * System take available optimizations\n",
    "* Can lead to horrible errors\n",
    "  * Because semantics/correctness changes based on job configuration.  Dangerous!\n",
    "\n",
    "\n",
    "**Standard Mode DANGER**\n",
    "\n",
    "* You develop program on small cluster\n",
    "  * Has plenty of memory for small instances\n",
    "  * Messages get buffered which hides unsafe (deadlock) messaging protocol\n",
    "* You launch code on big cluster with big instance\n",
    "  * Bigger messages and more memory consumption means that MPI can’t buffer messages\n",
    "  * Standard mode falls back to synchronous sends\n",
    "  * Your code breaks\n",
    "\n",
    "_Best practice_: test messaging protocols with synchronous sends, deploy code in standard mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messaging Topologies\n",
    "\n",
    "There are many strategies for breaking deadlock.\n",
    "\n",
    "### One Receiver\n",
    "\n",
    "For linear orderings and rings\n",
    "* Simplest and sufficient: (n-1) send/receive, 1 receive/send\n",
    "* Correct for any connected topology, any number of nodes\n",
    "```c\n",
    "// Example for a ring\n",
    "next =  ( ID + 1 ) % num_procs;\n",
    "prev = ID == 0 ? num_procs -1 : ID-1; \n",
    "if ( ID==0 )\n",
    "    receive(source=prev);\n",
    "    send(target=next);\n",
    "} else  {  \n",
    "    send(target=next);\n",
    "    receive(source=prev);\n",
    "}\n",
    "```\n",
    "This discipline is used by the `MPI_send_receive()` call.  It's always correct, but creates a serial dependency among messages.  The following chart shows the outcome of send calls (yellow) and receive calls (blue)\n",
    "\n",
    "<img src=\"http://pages.tacc.utexas.edu/~eijkhout/pcse/html/graphics/linear-serial.jpg\" width=\"400\" />\n",
    "\n",
    "### Paired Sends and Receives\n",
    "\n",
    "Order/pair sends and receives to avoid deadlocks (see `passitforward.c`)\n",
    " * More parallel alternative\n",
    " * Break two-way communication into two phases in which half of the nodes send in phase one and receive in phase two and vice version\n",
    "\n",
    "<img src=\"./images/pairedsr.png\" width=512 />\n",
    "\n",
    "#### More complex communication topologies?\n",
    "\n",
    "How would we design a messaging discipline for a continuous/cyclic 2-d grid\n",
    "of processes?\n",
    "  * That want to send to up/down/left/right neighbors?\n",
    "  * That want to send to diagonal neighbors also?\n",
    "  * What assumptions does it make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xeus-cling-cpp11"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "-std=c++11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
