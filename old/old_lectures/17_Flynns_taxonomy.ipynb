{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flynn's Taxonomy \n",
    "\n",
    "Characterize machines by number of instruction streams and data streams\n",
    "  * Defined in 1972.  Still common practice.\n",
    "  * A little too restrictive, but a starting place\n",
    "\n",
    "### SISD: single instruction, single data\n",
    "\n",
    "<img src=\"./images/sisd.png\" width=\"300\" title=\"SISD http://arstechnica.com/paedia/c/cpu/part-1/cpu1-1.html\" /><img src=\"./images/vn.png\" width=\"300\" title=\"Unknown source.\" />\n",
    "\n",
    "Perform  a stream on instructions on a stream of data\n",
    "  * The von Neumann architecture\n",
    "  * Conforms to serial algorithmic analysis\n",
    "\n",
    "### SIMD: single instruction, multiple data\n",
    "\n",
    "<img src=\"./images/simd.png\" width=\"512\" title=\"SISD http://arstechnica.com/paedia/c/cpu/part-1/cpu1-1.html\" />\n",
    "\n",
    "* Single control stream\n",
    "  * All processors execute the same instruction at the same time\n",
    "  * against different data\n",
    "  * synchronous execution or 'in lock step'\n",
    "* Fine-grained parallelism without inter-process communication\n",
    "* Limited examples  \n",
    "  * Intel vector processors\n",
    "  * GPU stream processor\n",
    "\n",
    "The examples are limited because these are SIMD computing elements which \n",
    "are part of a more complex heterogeneous system.\n",
    "\n",
    "### MISD: multiple instruction, single data\n",
    "\n",
    "As a taxon, this class is irrelevant.  No such machines.\n",
    "  * Machines called systolic arrays used some principles\n",
    "  * The Google TPU has some of these principles.\n",
    "The concept may get used in more complex systems.\n",
    "\n",
    "### MIMD: multiple instruction, multiple data\n",
    "\n",
    "<img src=\"./images/mimd.png\" width=\"512\" title=\"Unknown source.\" />\n",
    "\n",
    "* Asycnchronous parallelism\n",
    "  * Mutliple processing units\n",
    "  * Each processing unit gets independent input/data streams\n",
    "* Most of the machines that we are interested in are MIMD + complex processing units\n",
    "  * GPU is a MIMD of SIMD processors\n",
    "  * NUMA is a MIMD of MIMD proessors\n",
    "  * HPC is a MIMD of MIMD of MIMD and SIMD processors\n",
    "  \n",
    "Flynnâ€™s taxonomy not so useful.  We ust further divide the world by architectural features and programming model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Memory Systems\n",
    "\n",
    "<img src=\"https://computing.llnl.gov/tutorials/parallel_comp/images/distributed_mem.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture\" />\n",
    "\n",
    "MIMD machines in which the processing units have their own private memory.  The only way to share data among nodes is to moved it explicitly from one memory to another.\n",
    "* _Message passing_\n",
    "  * A programming pattern in which parallel nodes intermix computation with sending and receiving data from other nodes.\n",
    "  * MPI (message-passing interface) is the standard programming environment to implement this pattern.\n",
    "* Cloud frameworks create programming environments that map computations to distributed memory\n",
    "  * Hadoop!(Map/Reduce): data-parallel programs over file I/O\n",
    "  * Spark: data-parallel programs over memory-resident partitioned data structures\n",
    "  \n",
    "### Hybrid Architectures\n",
    "\n",
    "\n",
    "<img src=\"https://computing.llnl.gov/tutorials/parallel_comp/images/hybrid_mem2.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture\" />\n",
    "\n",
    "Hierarchical combination of multiple architectures.\n",
    "* Simple cloud or simple cluster \n",
    "  * distributed memory system consisting of\n",
    "  * SMP or NUMA nodes.\n",
    "* HPC or machine learning cloud\n",
    "  * distributed memory system consisting of\n",
    "  * SMP or NUMA nodes\n",
    "  * that have GPU, FPGA, Sunway, Intel MIC accelerators\n",
    "  \n",
    "The key challenge in modern parallel programming is efficiently mapping problems into codes that run efficiently on hybrid architectures.\n",
    "* The HPC approach is to write an MPI program that runs OpenMP programs on each node and makes calls to accelerators\n",
    "  * This is expensive and hard to maintain\n",
    "  * But a general solution and required to utilize hardware well\n",
    "* Machine learning frameworks have recently made great strides toward transparent support for hybrid systems\n",
    "  * PyTorch: NUMA+GPU\n",
    "  * MxNet: cloud+NUMA+GPU\n",
    "  * a great path forward for compute-dense iterative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
