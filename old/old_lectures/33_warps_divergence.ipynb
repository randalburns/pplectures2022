{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warps and Loop Unrolling\n",
    "\n",
    "### Review Data Decomposition\n",
    "\n",
    "CUDA defines an equivalence between data (_grid_) and execution (_thread_).\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/grid-of-thread-blocks.png\" />\n",
    "\n",
    "The programmer models their problem as a grid of data for which one thread is allocated per cell.  This is largely dictated by the requirement for coalesced memory access.\n",
    "\n",
    "The __Thread Block__ is an intermediate level of decomposition that runs on a single stream multiprocessor.\n",
    "\n",
    "### Scheduling Threads (Warps)\n",
    "\n",
    "<img src=\"https://www.3dgep.com/wp-content/uploads/2011/11/Dual-Warp-Scheduler.png\" width=512 />\n",
    "\n",
    "* CUDA threads are actually mapped onto hardware 32 threads at a time:\n",
    "  * concurrent launch of 16 thread half-warp\n",
    "  * half-warp matches the cache line size, i.e. if each thread reads/writes a contiguous element and the access is aligned, it is coalesced.\n",
    "  * so memory architecture dictates scheduling\n",
    "* Interleaving multiple warps allows longer running instructions one per clock cycle\n",
    "  * instruction execution actually takes many clock cycles\n",
    "  * same principle as processor pipelining\n",
    "  \n",
    "Warp execution is the __SIMD__ in GPU.  All threads do exactly the same thing at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What this means for unrolling?\n",
    "\n",
    "Let's consider an inner loop of a CUDA kernel operating on shared memory that performing a reduction:\n",
    "  * use half as many threads in each iteration (from a thread block down to 2).\n",
    "  * merging results to thread 0\n",
    "  * synchronize across thread blocks on each iteration\n",
    "\n",
    "```c\n",
    "for (unsigned int j=blockDim.x >> 1; j>0; j>>=1)\n",
    "{\n",
    "  if (tid < j)\n",
    "    SharedData[tid] += SharedData[tid+j];\n",
    "  __syncthreads();\n",
    "}\n",
    "```\n",
    "\n",
    "And unroll the loop fully\n",
    "\n",
    "```c\n",
    "if (tid < 128)\n",
    "    SharedData[tid] += SharedData[tid+128];\n",
    "__syncthreads(); \n",
    "if (tid < 64)\n",
    "    SharedData[tid] += SharedData[tid+64];\n",
    "__syncthreads();\n",
    "\n",
    "...\n",
    "\n",
    "if (tid < 1)\n",
    "    SharedData[tid] += SharedData[tid+1];\n",
    "__syncthreads();\n",
    "```\n",
    "\n",
    "OK, but what do we know about 32 or fewer threads:\n",
    "  * only thread 0-31 are active\n",
    "  * the operate in a warp\n",
    "  * the SIMD property guarantees that they are synchronous\n",
    "  * remove `__syncthreads()`\n",
    "  \n",
    "```c\n",
    "if (tid < 128)\n",
    "    SharedData[tid] += SharedData[tid+128];\n",
    "__syncthreads();\n",
    "if (tid < 64)\n",
    "    SharedData[tid] += SharedData[tid+64];\n",
    "__syncthreads();\n",
    "\n",
    "if (tid < 32)\n",
    "{\n",
    "  SharedData[tid] += SharedData[tid+32];\n",
    "  SharedData[tid] += SharedData[tid+16];\n",
    "  SharedData[tid] += SharedData[tid+8];\n",
    "  SharedData[tid] += SharedData[tid+4];\n",
    "  SharedData[tid] += SharedData[tid+2];\n",
    "  SharedData[tid] += SharedData[tid+1];\n",
    "}\n",
    "```\n",
    "\n",
    "* We just eliminated:\n",
    "    * 5 branching `if` statements\n",
    "    * 6 barriers (with contention)\n",
    "    * 1/3 of all instructions in the kernel\n",
    "* 1/3 seems like way too many, how is that possible?\n",
    "    * a single instruction gets charged against the whole warp\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divergence and Conditionals\n",
    "\n",
    "\n",
    "Conditional operators `if, for, do, while` are expensive in CUDA. They incur extra overhead to decide which threads in a warp run and which don't for simple conditionals.  It gets worse when there is __divergence__.\n",
    "\n",
    "When a SIMD warp faces an if statement in which threads diverge\n",
    "```c\n",
    "if (a[i] > C)\n",
    "{\n",
    "    action;\n",
    "} else {\n",
    "    otheraction;\n",
    "}\n",
    "```\n",
    "the two actions are serialized.  \n",
    "  * the test `if` runs as an instruction\n",
    "  * `action` runs on threads that pass\n",
    "  * then `otheraction` runs on other threads\n",
    "  \n",
    "  \n",
    "### Eliminating Conditionals\n",
    "\n",
    "Many conditional statements can be eliminated.\n",
    "\n",
    "* By loop unrolling \n",
    "* with predicated code\n",
    "  * converted a conditional to a predicate and operation\n",
    "  * all threads execute and get differet result\n",
    "  * this is a cool and powerful patter.\n",
    "  \n",
    "```c\n",
    "  /* branch version */\n",
    "  if ( src[i] < V )\n",
    "    j++;\n",
    "\n",
    "  /* predicated version */\n",
    "  bool b = (src[i] < V);\n",
    "  j+=b;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
