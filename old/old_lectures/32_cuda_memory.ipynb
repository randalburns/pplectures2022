{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Memory Usage\n",
    "\n",
    "Lecture derived from https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Access Patterns\n",
    "\n",
    "  \n",
    "#### Coalesced Memory Accesses\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/graphics/coalesced-access.png\" />\n",
    "in which all threads access a single cache line.  The CUDA warp (SIMD execution context) does a completely parallel transfer in a single memory access. Similar access patters may take twice as long:\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/graphics/unaligned-sequential-addresses.png\" />\n",
    "\n",
    "The following happens as two memory references.  The effect of this can be seen in this kernel example\n",
    "\n",
    "```c\n",
    "__global__ void offsetCopy(float *odata, float* idata, int offset)\n",
    "{\n",
    "    int xid = blockIdx.x * blockDim.x + threadIdx.x + offset;\n",
    "    odata[xid] = idata[xid];\n",
    "}\n",
    "```\n",
    "\n",
    "which produces the following latency chart.  Every 32 offsets, it gets lucky and is __aligned__ and realizes higher throughput.  Memory allocations in CUDA are aligned.  So, programs that allocate data and use it sequential in line sizes are aligned.\n",
    "\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/graphics/performance-of-offsetcopy-kernel.png\" />\n",
    "\n",
    "#### Banked Memory\n",
    "\n",
    "CUDA memory is _banked_ and caches are _direct mapped_.\n",
    "\n",
    "Only one thread at a time can access memory at a given bank offest and every memory address associates with a single bank offest---this is direct mapped.  So, when accessing data with a stride of 2, we get\n",
    "half the throughput.  The following image demonstrates a bank conflict and requires two memory access to read the data.\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/graphics/adjacent-threads-accessing-memory-with-stride-of-2.png\" />\n",
    "\n",
    "This results in a throughput collapse when accessing strided data.  At strides of 32, one gets only one word per cache line and uses 32 accesses per warp to get the data.\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/graphics/performance-of-stridecopy-kernel.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Chip and Off Chip Memory\n",
    "\n",
    "CUDA has a very little managed cache (an inconsequential amount of L1 and L2).  However, modest amounts of \"programmable\" cache, aka _shared memory_ are available on each stream processor.  To a first order, think of memory as being either on-chip (shared-memory and registers) or off-chip (all other).\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/graphics/memory-spaces-on-cuda-device.png\" />\n",
    "\n",
    "From the perspective of Roofline performance, transfers from device/texture/read-only memory competes for the off-chip memory bandwidth and limit performance.  In contrast, references to on-chip memory increase operational intensity when they replace off-chip access; same amount of computing with fewer off-chip acesses.\n",
    "\n",
    "#### Registers\n",
    "\n",
    "You the programmer cannot program the registers. The are used by the compiler to place data. There are some programming practices that make it so that there is lots of data for the compiler to put in registers.  You should think of this as fast scratch space for the compiler.\n",
    "\n",
    "__GPUs have many registers__ (1024 in Volta) in comparison with CPUs (10s).\n",
    "\n",
    "\n",
    "#### Shared Memory\n",
    "\n",
    "This is fast scratch space for the prorgammer.  Let's consider a 2-d multiply kernel\n",
    "that multiplies a row of `a[]` with a column of `b[]`.\n",
    "\n",
    "```c\n",
    "__global__ void simpleMultiply(float *a, float* b, float *c,\n",
    "                               int N)\n",
    "{\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float sum = 0.0f;\n",
    "    for (int i = 0; i < TILE_DIM; i++) {\n",
    "        sum += a[row*TILE_DIM+i] * b[i*N+col];\n",
    "    }\n",
    "    c[row*N+col] = sum;\n",
    "}\n",
    "```\n",
    "\n",
    "All references to `a[]` and `b[]` go _off-chip_ and this realizes only $6.6 GB/s$.\n",
    "\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/graphics/matrix-multiplication-block-column-by-block-row.png\" />\n",
    "\n",
    "This kernel can be re-written to explicitly load `a[]` into shared memory so that the off-chip load is coaslesced and then the element access are done on-chip.  This does not reduce the total amount of data loaded, it just loads it better, realizing $7.8 GB/s$.  Access to `b[]` are coalesced across threads.\n",
    "\n",
    "```c\n",
    "__global__ void coalescedMultiply(float *a, float* b, float *c,\n",
    "                                  int N)\n",
    "{\n",
    "    __shared__ float aTile[TILE_DIM][TILE_DIM];\n",
    "\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float sum = 0.0f;\n",
    "    aTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];\n",
    "    for (int i = 0; i < TILE_DIM; i++) {\n",
    "        sum += aTile[threadIdx.y][i]* b[i*N+col];\n",
    "    }\n",
    "    c[row*N+col] = sum;\n",
    "}\n",
    "```\n",
    "\n",
    "The preferred implementation collaborates among all threads in the `ThreadBlock` to load the \n",
    "data of a tile once and then only locally. This doubles performance $14.9 GB/s$.\n",
    "\n",
    "```c\n",
    "__global__ void sharedABMultiply(float *a, float* b, float *c,\n",
    "                                 int N)\n",
    "{\n",
    "    __shared__ float aTile[TILE_DIM][TILE_DIM],\n",
    "                     bTile[TILE_DIM][TILE_DIM];\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    float sum = 0.0f;\n",
    "    aTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];\n",
    "    bTile[threadIdx.y][threadIdx.x] = b[threadIdx.y*N+col];\n",
    "    __syncthreads();\n",
    "    for (int i = 0; i < TILE_DIM; i++) {\n",
    "        sum += aTile[threadIdx.y][i]* bTile[i][threadIdx.x];\n",
    "    }\n",
    "    c[row*N+col] = sum;\n",
    "}\n",
    "```\n",
    "\n",
    "This is the dominant patterin in shared memory.\n",
    "  * load all off-chip data needed collboratively\n",
    "  * call a barrier `syncthreads()` to make sure that all data are available\n",
    "  * compute using on-chip data\n",
    "  * write results off-chip `c[]`\n",
    "  \n",
    "#### Comparison with CPU\n",
    "\n",
    "In a CPU program, you would write a tiled implementation and choose the tile size that fits into a high-level processor cache, L1 or L2.  As the program runs, the data would be loaded into L1 (say) a line at a time when requested and all subsequent references would be fulfilled in the L1 cache.  All memory -> L1 accesses would be coalesced by the system.\n",
    "  * This is the diffderent between __managed__ cache (CPU) and __programmable__ cache (GPU).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Conclusions\n",
    "\n",
    "* CUDA has a restrictive memory model\n",
    "    * direct-mapped and banked requires strict coalesced access\n",
    "    * small-manage cache dictates use of shared memory\n",
    "* Programmer must be aware of architectural to get good performance\n",
    "    * break program into pieces that fit into shared memory\n",
    "    * consider alignment\n",
    "* Registers are important to performance\n",
    "    * managed by compiler\n",
    "    * programs can't write intermediate data back to memory without a severe performance penalty\n",
    "    * necessitates a large register file\n",
    "    * L1 data cache serves this role in CPU.\n",
    "    \n",
    "Off-chip memory throughput is the primary driver of performance.  The reading focuses on comparing kernel realized throughput to specified HW throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
